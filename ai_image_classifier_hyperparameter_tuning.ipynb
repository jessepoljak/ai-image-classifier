{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "import tensorflow as tf\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scorer accuracy\n",
    "score_acc = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pkl file.\n",
    "with open('ai_image_classifier_small_one_aug_img.pkl', 'rb') as file:\n",
    "    recalled_imgs = pickle.load(file)\n",
    "\n",
    "type(recalled_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train, test variables.\n",
    "X_train = recalled_imgs['X_train']\n",
    "X_test = recalled_imgs['X_test']\n",
    "y_train = recalled_imgs['y_train']\n",
    "y_test = recalled_imgs['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values to numpy arrays\n",
    "X_train_aug_np = np.array(X_train)\n",
    "X_test_np = np.array(X_test)\n",
    "y_train_aug_np = np.array(y_train) \n",
    "y_test_np = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for additional layers\n",
    "# Create function\n",
    "def nn_cl_bo2(neurons, filter1, filter2, activation, optimizer, learning_rate, batch_size, epochs,\n",
    "              layers1, layers2, normalization, dropout, dropout_rate):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD= {'Adam':Adam(learning_rate=learning_rate), 'SGD':SGD(learning_rate=learning_rate),\n",
    "                 'RMSprop':RMSprop(learning_rate=learning_rate), 'Adadelta':Adadelta(learning_rate=learning_rate),\n",
    "                 'Adagrad':Adagrad(learning_rate=learning_rate), 'Adamax':Adamax(learning_rate=learning_rate),\n",
    "                 'Nadam':Nadam(learning_rate=learning_rate), 'Ftrl':Ftrl(learning_rate=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons = round(neurons)\n",
    "    if round(filter1)/32 > 1.5:\n",
    "        first_layer_filter = 64\n",
    "    else:\n",
    "        first_layer_filter = 32\n",
    "    if round(filter2)/32 > 1.5:\n",
    "        second_layer_filter = 64\n",
    "    else:\n",
    "        second_layer_filter = 32\n",
    "    activation = activationL[round(activation)]\n",
    "    optimizer = optimizerD[optimizerL[round(optimizer)]]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    layers1 = round(layers1)\n",
    "    layers2 = round(layers2)\n",
    "    def nn_cl_fun():\n",
    "        nn = Sequential()\n",
    "        nn.add(layers.Conv2D(first_layer_filter, (3, 3), activation=activation, input_shape=(64, 64, 3)))\n",
    "        nn.add(layers.MaxPooling2D((2, 2)))\n",
    "        if normalization > 0.5:\n",
    "            nn.add(layers.BatchNormalization())\n",
    "        for i in range(layers1):\n",
    "            nn.add(layers.Conv2D(second_layer_filter, (3,3), activation=activation))\n",
    "            nn.add(layers.MaxPooling2D((2, 2)))\n",
    "        if dropout > 0.5:\n",
    "            nn.add(layers.Dropout(dropout_rate, seed=123))\n",
    "        nn.add(layers.Flatten())\n",
    "        for i in range(layers2):\n",
    "            nn.add(layers.Dense(neurons, activation=activation))\n",
    "        nn.add(layers.Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]})\n",
    "    score=np.nan_to_num(score)\n",
    "    score=score.mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  dropout  | dropou... |  epochs   |  filter1  |  filter2  |  layers1  |  layers2  | learni... |  neurons  | normal... | optimizer |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.1011   \u001b[39m | \u001b[39m5.51     \u001b[39m | \u001b[39m60.4     \u001b[39m | \u001b[39m0.4361   \u001b[39m | \u001b[39m0.2308   \u001b[39m | \u001b[39m12.95    \u001b[39m | \u001b[39m36.77    \u001b[39m | \u001b[39m32.72    \u001b[39m | \u001b[39m1.84     \u001b[39m | \u001b[39m1.477    \u001b[39m | \u001b[39m0.3443   \u001b[39m | \u001b[39m99.16    \u001b[39m | \u001b[39m0.2377   \u001b[39m | \u001b[39m0.5683   \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.1011   \u001b[39m | \u001b[39m6.026    \u001b[39m | \u001b[39m136.4    \u001b[39m | \u001b[39m0.2743   \u001b[39m | \u001b[39m0.1399   \u001b[39m | \u001b[39m11.18    \u001b[39m | \u001b[39m34.37    \u001b[39m | \u001b[39m60.82    \u001b[39m | \u001b[39m2.588    \u001b[39m | \u001b[39m2.681    \u001b[39m | \u001b[39m0.8171   \u001b[39m | \u001b[39m99.19    \u001b[39m | \u001b[39m0.5773   \u001b[39m | \u001b[39m5.696    \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.1584   \u001b[39m | \u001b[35m3.792    \u001b[39m | \u001b[35m36.61    \u001b[39m | \u001b[35m0.4541   \u001b[39m | \u001b[35m0.0316   \u001b[39m | \u001b[35m18.17    \u001b[39m | \u001b[35m54.33    \u001b[39m | \u001b[35m50.09    \u001b[39m | \u001b[35m1.548    \u001b[39m | \u001b[35m2.997    \u001b[39m | \u001b[35m0.1467   \u001b[39m | \u001b[35m65.39    \u001b[39m | \u001b[35m0.485    \u001b[39m | \u001b[35m2.835    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.1011   \u001b[39m | \u001b[39m6.551    \u001b[39m | \u001b[39m86.23    \u001b[39m | \u001b[39m0.4005   \u001b[39m | \u001b[39m0.09569  \u001b[39m | \u001b[39m19.47    \u001b[39m | \u001b[39m61.4     \u001b[39m | \u001b[39m58.04    \u001b[39m | \u001b[39m1.068    \u001b[39m | \u001b[39m2.887    \u001b[39m | \u001b[39m0.9509   \u001b[39m | \u001b[39m82.59    \u001b[39m | \u001b[39m0.4813   \u001b[39m | \u001b[39m6.767    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.09891  \u001b[39m | \u001b[39m3.753    \u001b[39m | \u001b[39m85.57    \u001b[39m | \u001b[39m0.01644  \u001b[39m | \u001b[39m0.01145  \u001b[39m | \u001b[39m10.53    \u001b[39m | \u001b[39m36.09    \u001b[39m | \u001b[39m33.07    \u001b[39m | \u001b[39m1.453    \u001b[39m | \u001b[39m2.088    \u001b[39m | \u001b[39m0.1868   \u001b[39m | \u001b[39m26.14    \u001b[39m | \u001b[39m0.1496   \u001b[39m | \u001b[39m4.781    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.09891  \u001b[39m | \u001b[39m4.221    \u001b[39m | \u001b[39m161.1    \u001b[39m | \u001b[39m0.6688   \u001b[39m | \u001b[39m0.1324   \u001b[39m | \u001b[39m15.95    \u001b[39m | \u001b[39m46.77    \u001b[39m | \u001b[39m62.82    \u001b[39m | \u001b[39m1.296    \u001b[39m | \u001b[39m2.814    \u001b[39m | \u001b[39m0.5029   \u001b[39m | \u001b[39m63.32    \u001b[39m | \u001b[39m0.6847   \u001b[39m | \u001b[39m1.242    \u001b[39m |\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m0.1613   \u001b[39m | \u001b[35m7.782    \u001b[39m | \u001b[35m113.0    \u001b[39m | \u001b[35m0.06934  \u001b[39m | \u001b[35m0.1094   \u001b[39m | \u001b[35m13.62    \u001b[39m | \u001b[35m47.11    \u001b[39m | \u001b[35m37.74    \u001b[39m | \u001b[35m1.351    \u001b[39m | \u001b[35m2.898    \u001b[39m | \u001b[35m0.6591   \u001b[39m | \u001b[35m22.15    \u001b[39m | \u001b[35m0.9278   \u001b[39m | \u001b[35m5.893    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.1011   \u001b[39m | \u001b[39m0.475    \u001b[39m | \u001b[39m66.22    \u001b[39m | \u001b[39m0.8725   \u001b[39m | \u001b[39m0.2022   \u001b[39m | \u001b[39m12.97    \u001b[39m | \u001b[39m55.5     \u001b[39m | \u001b[39m51.79    \u001b[39m | \u001b[39m1.421    \u001b[39m | \u001b[39m2.604    \u001b[39m | \u001b[39m0.06852  \u001b[39m | \u001b[39m37.74    \u001b[39m | \u001b[39m0.2074   \u001b[39m | \u001b[39m6.805    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.1072   \u001b[39m | \u001b[39m8.185    \u001b[39m | \u001b[39m145.6    \u001b[39m | \u001b[39m0.1152   \u001b[39m | \u001b[39m0.2874   \u001b[39m | \u001b[39m10.92    \u001b[39m | \u001b[39m52.15    \u001b[39m | \u001b[39m60.89    \u001b[39m | \u001b[39m1.68     \u001b[39m | \u001b[39m2.306    \u001b[39m | \u001b[39m0.9259   \u001b[39m | \u001b[39m43.66    \u001b[39m | \u001b[39m0.7714   \u001b[39m | \u001b[39m6.273    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.09891  \u001b[39m | \u001b[39m5.582    \u001b[39m | \u001b[39m57.38    \u001b[39m | \u001b[39m0.3749   \u001b[39m | \u001b[39m0.1908   \u001b[39m | \u001b[39m14.59    \u001b[39m | \u001b[39m49.03    \u001b[39m | \u001b[39m43.27    \u001b[39m | \u001b[39m2.32     \u001b[39m | \u001b[39m1.953    \u001b[39m | \u001b[39m0.46     \u001b[39m | \u001b[39m21.23    \u001b[39m | \u001b[39m0.4125   \u001b[39m | \u001b[39m1.912    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.1527   \u001b[39m | \u001b[39m3.121    \u001b[39m | \u001b[39m195.7    \u001b[39m | \u001b[39m0.2156   \u001b[39m | \u001b[39m0.2049   \u001b[39m | \u001b[39m10.32    \u001b[39m | \u001b[39m58.73    \u001b[39m | \u001b[39m54.45    \u001b[39m | \u001b[39m2.932    \u001b[39m | \u001b[39m2.184    \u001b[39m | \u001b[39m0.2252   \u001b[39m | \u001b[39m74.73    \u001b[39m | \u001b[39m0.03087  \u001b[39m | \u001b[39m2.931    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.09891  \u001b[39m | \u001b[39m2.531    \u001b[39m | \u001b[39m49.85    \u001b[39m | \u001b[39m0.4263   \u001b[39m | \u001b[39m0.2522   \u001b[39m | \u001b[39m11.1     \u001b[39m | \u001b[39m63.57    \u001b[39m | \u001b[39m39.47    \u001b[39m | \u001b[39m2.443    \u001b[39m | \u001b[39m2.322    \u001b[39m | \u001b[39m0.08698  \u001b[39m | \u001b[39m72.76    \u001b[39m | \u001b[39m0.2653   \u001b[39m | \u001b[39m6.313    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.1011   \u001b[39m | \u001b[39m7.364    \u001b[39m | \u001b[39m99.07    \u001b[39m | \u001b[39m0.8203   \u001b[39m | \u001b[39m0.05934  \u001b[39m | \u001b[39m15.23    \u001b[39m | \u001b[39m45.2     \u001b[39m | \u001b[39m61.63    \u001b[39m | \u001b[39m2.651    \u001b[39m | \u001b[39m2.002    \u001b[39m | \u001b[39m0.1256   \u001b[39m | \u001b[39m11.04    \u001b[39m | \u001b[39m0.9132   \u001b[39m | \u001b[39m3.587    \u001b[39m |\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.1625   \u001b[39m | \u001b[35m7.588    \u001b[39m | \u001b[35m47.93    \u001b[39m | \u001b[35m0.8034   \u001b[39m | \u001b[35m0.01867  \u001b[39m | \u001b[35m16.35    \u001b[39m | \u001b[35m63.0     \u001b[39m | \u001b[35m44.72    \u001b[39m | \u001b[35m2.319    \u001b[39m | \u001b[35m1.43     \u001b[39m | \u001b[35m0.891    \u001b[39m | \u001b[35m98.8     \u001b[39m | \u001b[35m0.7387   \u001b[39m | \u001b[35m4.49     \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.1559   \u001b[39m | \u001b[39m2.269    \u001b[39m | \u001b[39m111.9    \u001b[39m | \u001b[39m0.2681   \u001b[39m | \u001b[39m0.2605   \u001b[39m | \u001b[39m12.67    \u001b[39m | \u001b[39m63.14    \u001b[39m | \u001b[39m38.83    \u001b[39m | \u001b[39m1.596    \u001b[39m | \u001b[39m2.474    \u001b[39m | \u001b[39m0.02276  \u001b[39m | \u001b[39m12.72    \u001b[39m | \u001b[39m0.7338   \u001b[39m | \u001b[39m3.488    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.09891  \u001b[39m | \u001b[39m1.963    \u001b[39m | \u001b[39m33.5     \u001b[39m | \u001b[39m0.8507   \u001b[39m | \u001b[39m0.1609   \u001b[39m | \u001b[39m17.24    \u001b[39m | \u001b[39m53.17    \u001b[39m | \u001b[39m58.84    \u001b[39m | \u001b[39m1.776    \u001b[39m | \u001b[39m1.852    \u001b[39m | \u001b[39m0.8298   \u001b[39m | \u001b[39m52.21    \u001b[39m | \u001b[39m0.82     \u001b[39m | \u001b[39m0.09924  \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.108    \u001b[39m | \u001b[39m3.571    \u001b[39m | \u001b[39m53.56    \u001b[39m | \u001b[39m0.1181   \u001b[39m | \u001b[39m0.0618   \u001b[39m | \u001b[39m17.48    \u001b[39m | \u001b[39m61.76    \u001b[39m | \u001b[39m44.18    \u001b[39m | \u001b[39m1.921    \u001b[39m | \u001b[39m2.747    \u001b[39m | \u001b[39m0.8164   \u001b[39m | \u001b[39m12.25    \u001b[39m | \u001b[39m0.873    \u001b[39m | \u001b[39m4.232    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.1011   \u001b[39m | \u001b[39m4.648    \u001b[39m | \u001b[39m131.0    \u001b[39m | \u001b[39m0.7998   \u001b[39m | \u001b[39m0.05198  \u001b[39m | \u001b[39m11.87    \u001b[39m | \u001b[39m40.26    \u001b[39m | \u001b[39m33.63    \u001b[39m | \u001b[39m2.105    \u001b[39m | \u001b[39m1.704    \u001b[39m | \u001b[39m0.9178   \u001b[39m | \u001b[39m52.89    \u001b[39m | \u001b[39m0.1045   \u001b[39m | \u001b[39m1.068    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.1011   \u001b[39m | \u001b[39m2.462    \u001b[39m | \u001b[39m163.3    \u001b[39m | \u001b[39m0.3931   \u001b[39m | \u001b[39m0.2397   \u001b[39m | \u001b[39m14.33    \u001b[39m | \u001b[39m34.61    \u001b[39m | \u001b[39m32.34    \u001b[39m | \u001b[39m1.862    \u001b[39m | \u001b[39m2.326    \u001b[39m | \u001b[39m0.52     \u001b[39m | \u001b[39m62.21    \u001b[39m | \u001b[39m0.4931   \u001b[39m | \u001b[39m5.183    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.1011   \u001b[39m | \u001b[39m7.105    \u001b[39m | \u001b[39m163.2    \u001b[39m | \u001b[39m0.1275   \u001b[39m | \u001b[39m0.282    \u001b[39m | \u001b[39m18.78    \u001b[39m | \u001b[39m54.38    \u001b[39m | \u001b[39m61.98    \u001b[39m | \u001b[39m2.348    \u001b[39m | \u001b[39m2.189    \u001b[39m | \u001b[39m0.3516   \u001b[39m | \u001b[39m17.52    \u001b[39m | \u001b[39m0.5793   \u001b[39m | \u001b[39m0.1233   \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.1616   \u001b[39m | \u001b[39m4.972    \u001b[39m | \u001b[39m138.3    \u001b[39m | \u001b[39m0.6784   \u001b[39m | \u001b[39m0.05116  \u001b[39m | \u001b[39m15.77    \u001b[39m | \u001b[39m38.59    \u001b[39m | \u001b[39m40.05    \u001b[39m | \u001b[39m2.939    \u001b[39m | \u001b[39m2.793    \u001b[39m | \u001b[39m0.1316   \u001b[39m | \u001b[39m89.31    \u001b[39m | \u001b[39m0.948    \u001b[39m | \u001b[39m3.219    \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.1011   \u001b[39m | \u001b[39m3.717    \u001b[39m | \u001b[39m129.0    \u001b[39m | \u001b[39m0.3835   \u001b[39m | \u001b[39m0.04927  \u001b[39m | \u001b[39m16.49    \u001b[39m | \u001b[39m39.97    \u001b[39m | \u001b[39m53.09    \u001b[39m | \u001b[39m2.801    \u001b[39m | \u001b[39m2.815    \u001b[39m | \u001b[39m0.732    \u001b[39m | \u001b[39m25.75    \u001b[39m | \u001b[39m0.4175   \u001b[39m | \u001b[39m1.78     \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.1011   \u001b[39m | \u001b[39m6.634    \u001b[39m | \u001b[39m94.42    \u001b[39m | \u001b[39m0.4491   \u001b[39m | \u001b[39m0.2811   \u001b[39m | \u001b[39m18.13    \u001b[39m | \u001b[39m32.97    \u001b[39m | \u001b[39m40.21    \u001b[39m | \u001b[39m2.159    \u001b[39m | \u001b[39m1.767    \u001b[39m | \u001b[39m0.7801   \u001b[39m | \u001b[39m57.67    \u001b[39m | \u001b[39m0.116    \u001b[39m | \u001b[39m5.351    \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.147    \u001b[39m | \u001b[39m1.731    \u001b[39m | \u001b[39m95.14    \u001b[39m | \u001b[39m4.099e-05\u001b[39m | \u001b[39m0.2841   \u001b[39m | \u001b[39m16.99    \u001b[39m | \u001b[39m34.99    \u001b[39m | \u001b[39m62.06    \u001b[39m | \u001b[39m1.63     \u001b[39m | \u001b[39m1.519    \u001b[39m | \u001b[39m0.2717   \u001b[39m | \u001b[39m76.19    \u001b[39m | \u001b[39m0.8627   \u001b[39m | \u001b[39m0.3884   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.09891  \u001b[39m | \u001b[39m0.1048   \u001b[39m | \u001b[39m199.7    \u001b[39m | \u001b[39m0.4053   \u001b[39m | \u001b[39m0.2522   \u001b[39m | \u001b[39m10.78    \u001b[39m | \u001b[39m58.94    \u001b[39m | \u001b[39m32.71    \u001b[39m | \u001b[39m1.84     \u001b[39m | \u001b[39m1.937    \u001b[39m | \u001b[39m0.2345   \u001b[39m | \u001b[39m44.43    \u001b[39m | \u001b[39m0.3872   \u001b[39m | \u001b[39m5.974    \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.09891  \u001b[39m | \u001b[39m1.843    \u001b[39m | \u001b[39m66.22    \u001b[39m | \u001b[39m0.2104   \u001b[39m | \u001b[39m0.069    \u001b[39m | \u001b[39m15.97    \u001b[39m | \u001b[39m32.53    \u001b[39m | \u001b[39m63.68    \u001b[39m | \u001b[39m2.587    \u001b[39m | \u001b[39m1.553    \u001b[39m | \u001b[39m0.5452   \u001b[39m | \u001b[39m18.41    \u001b[39m | \u001b[39m0.1824   \u001b[39m | \u001b[39m5.357    \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.09891  \u001b[39m | \u001b[39m7.565    \u001b[39m | \u001b[39m140.9    \u001b[39m | \u001b[39m0.0555   \u001b[39m | \u001b[39m0.2834   \u001b[39m | \u001b[39m14.95    \u001b[39m | \u001b[39m58.16    \u001b[39m | \u001b[39m54.17    \u001b[39m | \u001b[39m2.845    \u001b[39m | \u001b[39m2.226    \u001b[39m | \u001b[39m0.7101   \u001b[39m | \u001b[39m95.45    \u001b[39m | \u001b[39m0.2476   \u001b[39m | \u001b[39m6.514    \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.1239   \u001b[39m | \u001b[39m8.3      \u001b[39m | \u001b[39m34.47    \u001b[39m | \u001b[39m0.3126   \u001b[39m | \u001b[39m0.0002843\u001b[39m | \u001b[39m12.08    \u001b[39m | \u001b[39m56.8     \u001b[39m | \u001b[39m36.76    \u001b[39m | \u001b[39m2.332    \u001b[39m | \u001b[39m2.153    \u001b[39m | \u001b[39m0.6339   \u001b[39m | \u001b[39m29.57    \u001b[39m | \u001b[39m0.2332   \u001b[39m | \u001b[39m5.044    \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.157    \u001b[39m | \u001b[39m8.129    \u001b[39m | \u001b[39m36.06    \u001b[39m | \u001b[39m0.2811   \u001b[39m | \u001b[39m0.2296   \u001b[39m | \u001b[39m11.17    \u001b[39m | \u001b[39m61.29    \u001b[39m | \u001b[39m35.17    \u001b[39m | \u001b[39m2.898    \u001b[39m | \u001b[39m2.908    \u001b[39m | \u001b[39m0.4316   \u001b[39m | \u001b[39m30.7     \u001b[39m | \u001b[39m0.596    \u001b[39m | \u001b[39m4.976    \u001b[39m |\n",
      "=====================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "params_nn2 ={\n",
    "    'neurons': (10, 100),\n",
    "    'filter1': (32, 64),\n",
    "    'filter2': (32, 64),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 0.10),\n",
    "    'batch_size':(32, 200),\n",
    "    'epochs':(10, 20),\n",
    "    'layers1':(1,3),\n",
    "    'layers2':(1,3),\n",
    "    'normalization':(0,1),\n",
    "    'dropout':(0,1),\n",
    "    'dropout_rate':(0,0.3),\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': <LeakyReLU name=leaky_re_lu_1, built=True>,\n",
       " 'batch_size': 48,\n",
       " 'dropout': 0.8034266175599356,\n",
       " 'dropout_rate': 0.01866844569344814,\n",
       " 'epochs': 16,\n",
       " 'filter1': 64,\n",
       " 'filter2': 32,\n",
       " 'layers1': 2,\n",
       " 'layers2': 1,\n",
       " 'learning_rate': 0.8909669645870872,\n",
       " 'neurons': 99,\n",
       " 'normalization': 0.7386745782934653,\n",
       " 'optimizer': <keras.src.optimizers.adagrad.Adagrad at 0x215ea69b130>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the best parameters.\n",
    "params_nn_ = nn_bo.max['params']\n",
    "learning_rate = params_nn_['learning_rate']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
    "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
    "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
    "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
    "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
    "if round(params_nn_['filter1'])/32 > 1.5:\n",
    "    params_nn_['filter1'] = 64\n",
    "else:\n",
    "    params_nn_['filter1'] = 32\n",
    "if round(params_nn_['filter2'])/32 > 1.5:\n",
    "    params_nn_['filter2'] = 64\n",
    "else:\n",
    "    params_nn_['filter2'] = 32\n",
    "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
    "optimizerD= {'Adam':Adam(learning_rate=learning_rate), 'SGD':SGD(learning_rate=learning_rate),\n",
    "             'RMSprop':RMSprop(learning_rate=learning_rate), 'Adadelta':Adadelta(learning_rate=learning_rate),\n",
    "             'Adagrad':Adagrad(learning_rate=learning_rate), 'Adamax':Adamax(learning_rate=learning_rate),\n",
    "             'Nadam':Nadam(learning_rate=learning_rate), 'Ftrl':Ftrl(learning_rate=learning_rate)}\n",
    "params_nn_['optimizer'] = optimizerD[optimizerL[round(params_nn_['optimizer'])]]\n",
    "params_nn_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
